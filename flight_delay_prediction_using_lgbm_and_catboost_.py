# -*- coding: utf-8 -*-
"""Flight Delay Prediction Using LGBM  and CatBoost .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uhYF3dNpZSGIYSCLmzk52DBAL4rwMpYB
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns

"""# Import Data"""

df = pd.read_csv('Flight_delay.csv')

df.head()

"""# Select Columns"""

df = df[['DayOfWeek','Date','DepTime','Airline','Origin','Dest','CarrierDelay']]
df

"""# Check for missing data"""

df.isnull().sum()

"""# Convert Date Column to Pandas Datetime"""

df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)

"""# Create Month And Day Feature"""

df['month'] = df['Date'].dt.month
df['day'] = df['Date'].dt.day

"""### Drop Date now"""

# Drop the original 'Date' column since XGBoost doesn't handle datetime types
df = df.drop(columns=['Date'])

"""# Identify Categorical variables"""

categories = df.select_dtypes(include=['object']).columns
categories

"""# One-Hot Encoding (Dummy Variables) for Categorical Data"""

df_encoded = pd.get_dummies(df, drop_first=True)

"""# Prepare Target Variable(s)"""

df_encoded['is_delayed_60+'] = np.where(df_encoded['CarrierDelay'] > 60, 1, 0)
# df_encoded['is_delayed_15+'] = np.where(df_encoded['CarrierDelay'] > 30, 1, 0)
df_encoded

"""# Define features and target variable

"""

X = df_encoded.drop(columns=['is_delayed_60+', 'CarrierDelay'])
y = df_encoded['is_delayed_60+']

X

"""# Split the data into training and testing sets"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

"""# LightGBM with Optuna - Handling Imbalanced with SMOTE + LGBM Class Weight

"""

# LightGBM + Optuna + SMOTE for Imbalanced Classification
import lightgbm as lgb
import optuna
from sklearn.model_selection import cross_val_score
from sklearn.metrics import roc_auc_score, classification_report
from imblearn.over_sampling import SMOTE

# Apply SMOTE on Training Data Only
print("Before SMOTE:", y_train.value_counts(normalize=True))
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)
print("After SMOTE:", y_train_res.value_counts(normalize=True))

# Objective Function for Optuna
def objective_lgb(trial):
    params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'verbosity': -1,
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),
        'num_leaves': trial.suggest_int('num_leaves', 20, 50),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),
        'class_weight': 'balanced',  # still keep balanced weights
        'random_state': 42
    }

    model = lgb.LGBMClassifier(**params)
    auc = cross_val_score(model, X_train_res, y_train_res, cv=3, scoring='roc_auc', n_jobs=-1).mean()
    return auc

# Run Optuna Optimization
study_lgb = optuna.create_study(direction='maximize', study_name='LightGBM_SMOTE')
study_lgb.optimize(objective_lgb, n_trials=30, show_progress_bar=True)

print("\nBest LightGBM Parameters:")
print(study_lgb.best_trial.params)

# Train Final Model with Best Parameters
best_params_lgb = study_lgb.best_trial.params
best_params_lgb['class_weight'] = 'balanced'

lgb_model = lgb.LGBMClassifier(**best_params_lgb)
lgb_model.fit(X_train_res, y_train_res)

# Evaluate Performance
y_pred_proba_lgb = lgb_model.predict_proba(X_test)[:, 1]
auc_lgb = roc_auc_score(y_test, y_pred_proba_lgb)

print(f"\nFinal LightGBM ROC-AUC: {auc_lgb:.4f}")

y_pred_lgb = lgb_model.predict(X_test)
print("\nClassification Report:\n", classification_report(y_test, y_pred_lgb))

# Plot the ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_lgb)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'AUC = {auc_lgb:.4f}')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

"""# CatBoost with Optuna"""

from catboost import CatBoostClassifier, Pool
import optuna
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import cross_val_score
from imblearn.over_sampling import SMOTE

# Apply SMOTE on Training Data Only
print("Before SMOTE:", y_train.value_counts(normalize=True))
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)
print("After SMOTE:", y_train_res.value_counts(normalize=True))

def objective_catboost(trial):
    params = {
        'iterations': trial.suggest_int('iterations', 30, 50),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),
        'depth': trial.suggest_int('depth', 4, 10),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-2, 10.0, log=True),
        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
        'border_count': trial.suggest_int('border_count', 32, 255),
        'random_strength': trial.suggest_float('random_strength', 0.1, 2.0),
        'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations', 1, 10),
        'loss_function': 'Logloss',
        'eval_metric': 'AUC',
        'verbose': False,
        'random_seed': 42
    }

    model = CatBoostClassifier(**params)

    # 3-fold cross-validation ROC-AUC
    auc = cross_val_score(model, X_train_res, y_train_res, cv=3, scoring='roc_auc', n_jobs=-1).mean()
    return auc

# Create study
study_cat = optuna.create_study(direction='maximize')
study_cat.optimize(objective_catboost, n_trials=30, show_progress_bar=True)

print("\nBest CatBoost Parameters:")
print(study_cat.best_trial.params)

# Train final CatBoost model
best_params_cat = study_cat.best_trial.params
cat_model = CatBoostClassifier(**best_params_cat)
cat_model.fit(X_train_res, y_train_res, verbose=False)

# Evaluate
y_pred_cat = cat_model.predict(X_test)
y_pred_proba_cat = cat_model.predict_proba(X_test)[:, 1]
auc_cat = roc_auc_score(y_test, y_pred_proba_cat)

print(f"\n Best CatBoost ROC-AUC on Test Set: {auc_cat:.4f}")

# Plot the ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_cat)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'AUC = {auc_cat:.4f}')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()